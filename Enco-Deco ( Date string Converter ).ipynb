{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba730a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5748ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# cannot use strftime()'s %B format since it depends on the locale\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "def random_dates(n_dates):\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "\n",
    "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "\n",
    "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
    "    y = [dt.isoformat() for dt in dates]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d02e13e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                    Target                   \n",
      "--------------------------------------------------\n",
      "September 20, 7075       7075-09-20               \n",
      "May 15, 8579             8579-05-15               \n",
      "January 11, 7103         7103-01-11               \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_dates = 3\n",
    "x_example, y_example = random_dates(n_dates)\n",
    "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
    "print(\"-\" * 50)\n",
    "for idx in range(n_dates):\n",
    "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961502c2",
   "metadata": {},
   "source": [
    "**Let's get the list of all possible characters in the inputs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d45fdbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ,0123456789ADFJMNOSabceghilmnoprstuvy'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
    "INPUT_CHARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53940345",
   "metadata": {},
   "source": [
    "**And here's the list of possible characters in the outputs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56392ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHARS = \"0123456789-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82c22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
    "    return [chars.index(c) for c in date_str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7112ede",
   "metadata": {},
   "source": [
    " **Writing a function to convert a string to a list of character IDs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5829d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 23, 31, 34, 23, 28, 21, 23, 32, 0, 4, 2, 1, 0, 9, 2, 9, 7]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(x_example[0], INPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fcc8da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 0, 7, 5, 10, 0, 9, 10, 2, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(y_example[0], OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bc96354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
    "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor() # using 0 as the padding token ID\n",
    "\n",
    "def create_dataset(n_dates):\n",
    "    x, y = random_dates(n_dates)\n",
    "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e3cadc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_train, Y_train = create_dataset(10000)\n",
    "X_valid, Y_valid = create_dataset(2000)\n",
    "X_test, Y_test = create_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecbf136a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 8,  1,  8,  6, 11,  1, 10, 11,  3,  1])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9db905d",
   "metadata": {},
   "source": [
    "**First version: a very basic seq2seq model\n",
    "Let's first try the simplest possible model: we feed in the input sequence, which first goes through the encoder (an embedding layer followed by a single LSTM layer), which outputs a vector, then it goes through a decoder (a single LSTM layer, followed by a dense output layer), which outputs a sequence of vectors, each representing the estimated probabilities for all possible output character.Since the decoder expects a sequence as input, we repeat the vector (which is output by the decoder) as many times as the longest possible output sequence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99a04e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 13s 25ms/step - loss: 1.8128 - accuracy: 0.3497 - val_loss: 1.3708 - val_accuracy: 0.4943\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.2090 - accuracy: 0.5590 - val_loss: 1.0403 - val_accuracy: 0.6230\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.9837 - accuracy: 0.6488 - val_loss: 0.8473 - val_accuracy: 0.6943\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.9595 - accuracy: 0.6654 - val_loss: 0.7428 - val_accuracy: 0.7227\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.6312 - accuracy: 0.7608 - val_loss: 0.5384 - val_accuracy: 0.7881\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.4527 - accuracy: 0.8222 - val_loss: 0.3747 - val_accuracy: 0.8536\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.6003 - accuracy: 0.7845 - val_loss: 0.3259 - val_accuracy: 0.8730\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 6s 21ms/step - loss: 0.3389 - accuracy: 0.8776 - val_loss: 0.2445 - val_accuracy: 0.9080\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.1941 - accuracy: 0.9361 - val_loss: 0.1625 - val_accuracy: 0.9489\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.1982 - accuracy: 0.9497 - val_loss: 0.1187 - val_accuracy: 0.9696\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0802 - accuracy: 0.9835 - val_loss: 0.0629 - val_accuracy: 0.9884\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0458 - accuracy: 0.9934 - val_loss: 0.0377 - val_accuracy: 0.9954\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.0277 - accuracy: 0.9974 - val_loss: 0.0237 - val_accuracy: 0.9981\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0396 - accuracy: 0.9940 - val_loss: 0.0257 - val_accuracy: 0.9974\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.0149 - accuracy: 0.9993 - val_loss: 0.0126 - val_accuracy: 0.9992\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.0087 - accuracy: 0.9999 - val_loss: 0.0085 - val_accuracy: 0.9997\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.0060 - accuracy: 0.9999 - val_loss: 0.0061 - val_accuracy: 0.9998\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 0.9990\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 6s 21ms/step - loss: 0.0038 - accuracy: 0.9999 - val_loss: 0.0038 - val_accuracy: 0.9998\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "max_output_length = Y_train.shape[1]\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1,\n",
    "                           output_dim=embedding_size,\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    encoder,\n",
    "    keras.layers.RepeatVector(max_output_length),\n",
    "    decoder\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, Y_train, epochs=20,\n",
    "                    validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a114b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
    "    return [\"\".join([(\"?\" + chars)[index] for index in sequence])\n",
    "            for sequence in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8545210",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs([\"November 10, 1970\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38255a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970-11-10\n",
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "#ids = model.predict_classes(X_new)\n",
    "ids = np.argmax(model.predict(X_new), axis=-1)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853bd17e",
   "metadata": {},
   "source": [
    "\n",
    "**However, since the model was only trained on input strings of length 18 (which is the length of the longest date), it does not perform well if we try to use it to make predictions on shorter sequences.We need to ensure that we always pass sequences of the same length as during training, using padding if necessary. Let's write a little helper function for that**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0b61b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = X_train.shape[1]\n",
    "\n",
    "def prepare_date_strs_padded(date_strs):\n",
    "    X = prepare_date_strs(date_strs)\n",
    "    if X.shape[1] < max_input_length:\n",
    "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
    "    return X\n",
    "\n",
    "def convert_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    #ids = model.predict_classes(X)\n",
    "    ids = np.argmax(model.predict(X), axis=-1)\n",
    "    return ids_to_date_strs(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74c67e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1970-03-01', '1789-07-14']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_date_strs([\"March 01, 1970\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704db05a",
   "metadata": {},
   "source": [
    "<h1>Feeding the shifted targets to the decoder (teacher forcing)</h1>\n",
    "\n",
    "**Instead of feeding the decoder a simple repetition of the encoder's output vector, we can feed it the target sequence, shifted by one time step to the right. This way, at each time step the decoder will know what the previous target character was. This should help is tackle more complex sequence-to-sequence problems.**\n",
    "\n",
    "**Since the first output character of each target sequence has no previous character, we will need a new token to represent the start-of-sequence (sos).**\n",
    "\n",
    "\n",
    "**let's create the decoder's inputs (for training, validation and testing). The sos token will be represented using the last possible output character's ID + 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "157b86ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def shifted_output_sequences(Y):\n",
    "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
    "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n",
    "\n",
    "X_train_decoder = shifted_output_sequences(Y_train)\n",
    "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
    "X_test_decoder = shifted_output_sequences(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d68c2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000, 10), dtype=int32, numpy=\n",
       "array([[12,  8,  1, ..., 10, 11,  3],\n",
       "       [12,  9,  6, ...,  6, 11,  2],\n",
       "       [12,  8,  2, ...,  2, 11,  2],\n",
       "       ...,\n",
       "       [12, 10,  8, ...,  2, 11,  4],\n",
       "       [12,  2,  2, ...,  3, 11,  3],\n",
       "       [12,  8,  9, ...,  8, 11,  3]])>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e35e0",
   "metadata": {},
   "source": [
    "<h1> Using the functional API </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f775f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 13s 24ms/step - loss: 1.6823 - accuracy: 0.3734 - val_loss: 1.4049 - val_accuracy: 0.4676\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.1923 - accuracy: 0.5552 - val_loss: 0.9020 - val_accuracy: 0.6647\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.6805 - accuracy: 0.7555 - val_loss: 0.4045 - val_accuracy: 0.8665\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.2329 - accuracy: 0.9396 - val_loss: 0.1511 - val_accuracy: 0.9654\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 6s 21ms/step - loss: 0.0772 - accuracy: 0.9907 - val_loss: 0.0428 - val_accuracy: 0.9987\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.0528 - accuracy: 0.9933 - val_loss: 0.0248 - val_accuracy: 0.9991\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.0170 - accuracy: 0.9998 - val_loss: 0.0143 - val_accuracy: 0.9998\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.0479 - accuracy: 0.9913 - val_loss: 0.0271 - val_accuracy: 0.9987\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.0117 - accuracy: 0.9999 - val_loss: 0.0081 - val_accuracy: 0.9999\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "lstm_units = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding = keras.layers.Embedding(\n",
    "    input_dim=len(INPUT_CHARS) + 1,\n",
    "    output_dim=encoder_embedding_size)(encoder_input)\n",
    "_, encoder_state_h, encoder_state_c = keras.layers.LSTM(\n",
    "    lstm_units, return_state=True)(encoder_embedding)\n",
    "encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding = keras.layers.Embedding(\n",
    "    input_dim=len(OUTPUT_CHARS) + 2,\n",
    "    output_dim=decoder_embedding_size)(decoder_input)\n",
    "decoder_lstm_output = keras.layers.LSTM(lstm_units, return_sequences=True)(\n",
    "    decoder_embedding, initial_state=encoder_state)\n",
    "decoder_output = keras.layers.Dense(len(OUTPUT_CHARS) + 1,\n",
    "                                    activation=\"softmax\")(decoder_lstm_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_input, decoder_input],\n",
    "                           outputs=[decoder_output])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=10,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1152689d",
   "metadata": {},
   "source": [
    "<h1>This time we need to predict characters one by one</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9775c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
    "    for index in range(max_output_length):\n",
    "        pad_size = max_output_length - Y_pred.shape[1]\n",
    "        X_decoder = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n",
    "        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n",
    "        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n",
    "        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n",
    "    return ids_to_date_strs(Y_pred[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50ce0889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f7bbb",
   "metadata": {},
   "source": [
    "<h1>Using TF-Addons's seq2seq implementation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "349971f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "313/313 [==============================] - 12s 23ms/step - loss: 1.6782 - accuracy: 0.3654 - val_loss: 1.4654 - val_accuracy: 0.4414\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 1.3855 - accuracy: 0.4594 - val_loss: 1.2149 - val_accuracy: 0.5408\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.9870 - accuracy: 0.6298 - val_loss: 0.8000 - val_accuracy: 0.7028\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.4387 - accuracy: 0.8555 - val_loss: 0.2132 - val_accuracy: 0.9531\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.1340 - accuracy: 0.9778 - val_loss: 0.0667 - val_accuracy: 0.9955\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.0941 - accuracy: 0.9857 - val_loss: 0.0469 - val_accuracy: 0.9973\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.0288 - accuracy: 0.9993 - val_loss: 0.0210 - val_accuracy: 0.9997\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 6s 21ms/step - loss: 0.0157 - accuracy: 0.9999 - val_loss: 0.0134 - val_accuracy: 0.9998\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 0.9999\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 6s 21ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 6s 21ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "units = 128\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "encoder_embeddings = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
    "\n",
    "decoder_embedding_layer = keras.layers.Embedding(\n",
    "    len(OUTPUT_CHARS) + 2, decoder_embedding_size)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(units)\n",
    "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n",
    "                                                 sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings,\n",
    "    initial_state=encoder_state)\n",
    "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                           outputs=[Y_proba])\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=15,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef57610f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1970-03-01', '2021-06-18']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"March 01, 1970\", \"June 18, 2021\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af9fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
